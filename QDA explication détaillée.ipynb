{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratic Discriminant Analysis (QDA) - Explication détaillée\n",
    "\n",
    "Le QDA est un algorithme de classification supervisée qui repose sur l’hypothèse que les données de chaque classe suivent une distribution gaussienne multivariée, mais avec des matrices de covariance distinctes pour chaque classe. Voici une décomposition étape par étape.\n",
    "\n",
    "---\n",
    "\n",
    "## Étape 1 : Initialisation du modèle\n",
    "### Objectif :\n",
    "Créer une classe `QDA` pour gérer les données nécessaires à l'entraînement et à la prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDA:\n",
    "    def __init__(self):\n",
    "        self.classes_ = None          # Liste des classes uniques\n",
    "        self.means_ = {}              # Moyenne des données pour chaque classe\n",
    "        self.covariances_ = {}        # Matrice de covariance pour chaque classe\n",
    "        self.priors_ = {}             # Probabilités a priori des classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`classes_`** : Contiendra les classes uniques dans les données d'entraînement.  \n",
    "- **`means_`** : Stocke les vecteurs de moyennes pour chaque classe.  \n",
    "- **`covariances_`** : Stocke les matrices de covariance pour chaque classe.  \n",
    "- **`priors_`** : Contient les probabilités a priori de chaque classe (fréquence relative dans les données).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 2 : Entraîner le modèle (`fit`)\n",
    "### Objectif :\n",
    "Calculer les paramètres nécessaires pour chaque classe à partir des données d’entraînement :  \n",
    "- Moyennes\n",
    "- Matrices de covariance\n",
    "- Probabilités a priori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y):\n",
    "    self.classes_ = np.unique(y)  # Trouver toutes les classes\n",
    "    for cls in self.classes_:\n",
    "        X_cls = X[y == cls]                           # Sélectionner les données de la classe\n",
    "        self.means_[cls] = np.mean(X_cls, axis=0)     # Moyenne des données\n",
    "        self.covariances_[cls] = np.cov(X_cls, rowvar=False)  # Matrice de covariance\n",
    "        self.priors_[cls] = X_cls.shape[0] / X.shape[0]       # Proportion des données pour la classe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Étapes principales** :  \n",
    "  1. Identifier les classes avec une méthode pour trouver les valeurs uniques dans les étiquettes.  \n",
    "  2. Pour chaque classe :  \n",
    "     - Filtrer les observations appartenant à cette classe.\n",
    "     - Calculer la **moyenne** des observations.\n",
    "     - Calculer la **matrice de covariance** (représente les dépendances entre les variables).  \n",
    "     - Calculer la probabilité a priori comme le ratio entre le nombre d'observations de cette classe et le nombre total d'observations.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 3 : Calculer la densité gaussienne multivariée (`_gaussian_density`)\n",
    "### Objectif :\n",
    "Définir une fonction pour calculer la densité de probabilité d’un vecteur dans une distribution gaussienne multivariée.\n",
    "\n",
    "#### Formule utilisée :\n",
    "$$\n",
    "P(x | y = k) = \\frac{1}{\\sqrt{(2\\pi)^d |\\Sigma|}} \\exp\\left(-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right)\n",
    "$$\n",
    "\n",
    "- $\\mu$  : vecteur de moyenne (par classe)  \n",
    "- $\\Sigma$ : matrice de covariance (par classe)  \n",
    "- $|\\Sigma|$ : déterminant de la matrice de covariance  \n",
    "- $\\Sigma^{-1}$ : inverse de la matrice de covariance  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gaussian_density(self, x, mean, covariance):\n",
    "    size = len(mean)  # Nombre de dimensions\n",
    "    epsilon = 1e-6    # Régularisation pour éviter une matrice singulière\n",
    "    covariance += np.eye(size) * epsilon\n",
    "\n",
    "    det = np.linalg.det(covariance)   # Déterminant de la matrice de covariance\n",
    "    inv_cov = np.linalg.inv(covariance)  # Inverse de la matri  ce de covariance\n",
    "\n",
    "    # Normalisation et exponentielle\n",
    "    norm_const = 1.0 / (np.power(2 * np.pi, size / 2) * np.sqrt(det))\n",
    "    x_diff = x - mean\n",
    "    return norm_const * np.exp(-0.5 * x_diff @ inv_cov @ x_diff.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Régularisation** : Ajout d’un petit terme à la diagonale de la matrice de covariance pour éviter des problèmes numériques si la matrice est singulière.\n",
    "- **Densité** : Calcule la probabilité d'une observation sous la distribution gaussienne définie par la moyenne et la matrice de covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 4 : Prédire la classe pour de nouvelles données (`predict`)\n",
    "### Objectif :\n",
    "Pour chaque observation, calculer la probabilité pour chaque classe, puis assigner la classe avec la probabilité maximale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    predictions = []\n",
    "    for x in X:\n",
    "        class_probs = {}\n",
    "        for cls in self.classes_:\n",
    "            # Probabilité conditionnelle pour chaque classe\n",
    "            likelihood = self._gaussian_density(x, self.means_[cls], self.covariances_[cls])\n",
    "            class_probs[cls] = likelihood * self.priors_[cls]  # Bayes: P(y|x) ∝ P(x|y)P(y)\n",
    "        # Classe avec la probabilité maximale\n",
    "        predictions.append(max(class_probs, key=class_probs.get))\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Étapes principales** :  \n",
    "  1. Pour chaque observation dans les données de test :  \n",
    "     - Calculer $P(x | y = k) \\cdot P(y = k)$ pour chaque classe $ k$.  \n",
    "  2. Choisir la classe $ k$ qui maximise cette valeur.  \n",
    "  3. Ajouter la prédiction à la liste des résultats.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé\n",
    "### Étapes principales du QDA :\n",
    "1. Calculer les **paramètres des distributions gaussiennes** pour chaque classe :\n",
    "   - Moyenne ($ \\mu $).\n",
    "   - Matrice de covariance ($ \\Sigma $).\n",
    "   - Probabilité a priori ($ P(y) $).\n",
    "\n",
    "2. Pour une observation donnée, estimer la **probabilité conditionnelle** pour chaque classe en appliquant la formule de densité gaussienne multivariée.\n",
    "\n",
    "3. **Classer** l’observation dans la classe ayant la probabilité maximale.\n",
    "\n",
    "---\n",
    "\n",
    "## Points importants\n",
    "- **Régularisation** : Nécessaire pour gérer les matrices singulières.  \n",
    "- **Normalité** : Le QDA repose sur l’hypothèse que les données suivent une distribution normale multivariée. Si ce n’est pas le cas, les résultats peuvent être sous-optimaux.  \n",
    "- **Complexité** : La matrice de covariance est inversée pour chaque classe, ce qui peut être coûteux pour des ensembles de données à forte dimension.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
